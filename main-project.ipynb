{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Risk Modeling Using Python\n",
    "## Based on online course from 365DataScience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import relevant libraries\n",
    "\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the datasets\n",
    "\n",
    "#first dataset contains the backup storage for the data, this will not be changed throughout the project, but only kept as backup in case of errors\n",
    "loan_data1 = pd.read_csv(\"data/3.1 loan_data_2007_2014 - 1.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_3556\\2830868825.py:1: DtypeWarning: Columns (20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  loan_data2 = pd.read_csv(\"data/3.1 loan_data_2007_2014 - 2.csv\")\n"
     ]
    }
   ],
   "source": [
    "loan_data2 = pd.read_csv(\"data/3.1 loan_data_2007_2014 - 2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_3556\\3320454629.py:1: DtypeWarning: Columns (20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  loan_data3 = pd.read_csv(\"data/3.1 loan_data_2007_2014 - 3.csv\",encoding = 'unicode_escape')\n"
     ]
    }
   ],
   "source": [
    "loan_data3 = pd.read_csv(\"data/3.1 loan_data_2007_2014 - 3.csv\",encoding = 'unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate these three files to one \n",
    "loan_data = pd.concat([loan_data1, loan_data2, loan_data3])\n",
    "pd.options.display.max_rows = 50\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 466272 entries, 0 to 173349\n",
      "Data columns (total 75 columns):\n",
      " #   Column                       Non-Null Count   Dtype  \n",
      "---  ------                       --------------   -----  \n",
      " 0   Unnamed: 0                   466272 non-null  int64  \n",
      " 1   id                           466272 non-null  int64  \n",
      " 2   member_id                    466272 non-null  int64  \n",
      " 3   loan_amnt                    466272 non-null  int64  \n",
      " 4   funded_amnt                  466272 non-null  int64  \n",
      " 5   funded_amnt_inv              466272 non-null  float64\n",
      " 6   term                         466272 non-null  object \n",
      " 7   int_rate                     466272 non-null  float64\n",
      " 8   installment                  466272 non-null  float64\n",
      " 9   grade                        466272 non-null  object \n",
      " 10  sub_grade                    466272 non-null  object \n",
      " 11  emp_title                    438684 non-null  object \n",
      " 12  emp_length                   445264 non-null  object \n",
      " 13  home_ownership               466272 non-null  object \n",
      " 14  annual_inc                   466268 non-null  float64\n",
      " 15  verification_status          466272 non-null  object \n",
      " 16  issue_d                      466272 non-null  object \n",
      " 17  loan_status                  466272 non-null  object \n",
      " 18  pymnt_plan                   466272 non-null  object \n",
      " 19  url                          466272 non-null  object \n",
      " 20  desc                         125981 non-null  object \n",
      " 21  purpose                      466272 non-null  object \n",
      " 22  title                        466252 non-null  object \n",
      " 23  zip_code                     466272 non-null  object \n",
      " 24  addr_state                   466272 non-null  object \n",
      " 25  dti                          466272 non-null  float64\n",
      " 26  delinq_2yrs                  466243 non-null  float64\n",
      " 27  earliest_cr_line             466243 non-null  object \n",
      " 28  inq_last_6mths               466243 non-null  float64\n",
      " 29  mths_since_last_delinq       215926 non-null  float64\n",
      " 30  mths_since_last_record       62637 non-null   float64\n",
      " 31  open_acc                     466243 non-null  float64\n",
      " 32  pub_rec                      466243 non-null  float64\n",
      " 33  revol_bal                    466272 non-null  int64  \n",
      " 34  revol_util                   465932 non-null  float64\n",
      " 35  total_acc                    466243 non-null  float64\n",
      " 36  initial_list_status          466272 non-null  object \n",
      " 37  out_prncp                    466272 non-null  float64\n",
      " 38  out_prncp_inv                466272 non-null  float64\n",
      " 39  total_pymnt                  466272 non-null  float64\n",
      " 40  total_pymnt_inv              466272 non-null  float64\n",
      " 41  total_rec_prncp              466272 non-null  float64\n",
      " 42  total_rec_int                466272 non-null  float64\n",
      " 43  total_rec_late_fee           466272 non-null  float64\n",
      " 44  recoveries                   466272 non-null  float64\n",
      " 45  collection_recovery_fee      466272 non-null  float64\n",
      " 46  last_pymnt_d                 465896 non-null  object \n",
      " 47  last_pymnt_amnt              466272 non-null  float64\n",
      " 48  next_pymnt_d                 239066 non-null  object \n",
      " 49  last_credit_pull_d           466230 non-null  object \n",
      " 50  collections_12_mths_ex_med   466127 non-null  float64\n",
      " 51  mths_since_last_major_derog  98973 non-null   float64\n",
      " 52  policy_code                  466272 non-null  int64  \n",
      " 53  application_type             466272 non-null  object \n",
      " 54  annual_inc_joint             0 non-null       float64\n",
      " 55  dti_joint                    0 non-null       float64\n",
      " 56  verification_status_joint    0 non-null       float64\n",
      " 57  acc_now_delinq               466243 non-null  float64\n",
      " 58  tot_coll_amt                 395997 non-null  float64\n",
      " 59  tot_cur_bal                  395997 non-null  float64\n",
      " 60  open_acc_6m                  0 non-null       float64\n",
      " 61  open_il_6m                   0 non-null       float64\n",
      " 62  open_il_12m                  0 non-null       float64\n",
      " 63  open_il_24m                  0 non-null       float64\n",
      " 64  mths_since_rcnt_il           0 non-null       float64\n",
      " 65  total_bal_il                 0 non-null       float64\n",
      " 66  il_util                      0 non-null       float64\n",
      " 67  open_rv_12m                  0 non-null       float64\n",
      " 68  open_rv_24m                  0 non-null       float64\n",
      " 69  max_bal_bc                   0 non-null       float64\n",
      " 70  all_util                     0 non-null       float64\n",
      " 71  total_rev_hi_lim             222647 non-null  float64\n",
      " 72  inq_fi                       0 non-null       float64\n",
      " 73  total_cu_tl                  0 non-null       float64\n",
      " 74  inq_last_12m                 0 non-null       float64\n",
      "dtypes: float64(46), int64(7), object(22)\n",
      "memory usage: 270.4+ MB\n"
     ]
    }
   ],
   "source": [
    "loan_data\n",
    "loan_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data = loan_data.drop(loan_data['annual_inc'].nlargest(10).index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_3556\\4143095900.py:5: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  loan_data[\"emp_length_int\"] = loan_data[\"emp_length\"].str.replace(\"\\+ years\", '')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocessing the continuous variables\n",
    "\n",
    "# delete the \"year\" or \"years\" from employement length variable\n",
    "loan_data['emp_length'].unique()\n",
    "loan_data[\"emp_length_int\"] = loan_data[\"emp_length\"].str.replace(\"\\+ years\", '')\n",
    "loan_data['emp_length_int'] = loan_data['emp_length_int'].str.replace('< 1 year', str(0))\n",
    "loan_data['emp_length_int'] = loan_data['emp_length_int'].str.replace('n/a', str(0))\n",
    "loan_data['emp_length_int'] = loan_data['emp_length_int'].str.replace(' years', '')\n",
    "loan_data['emp_length_int'] = loan_data['emp_length_int'].str.replace(' year', '')\n",
    "\n",
    "# transform the string into numeric\n",
    "loan_data[\"emp_length_int\"] = pd.to_numeric(loan_data[\"emp_length_int\"])\n",
    "\n",
    "# double check if the new variable is of a numeric type\n",
    "type(loan_data[\"emp_length_int\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([36, 60], dtype=int64)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transfor the term variable into numeric using the same steps as above\n",
    "loan_data[\"term\"].unique()\n",
    "loan_data[\"term_int\"] = loan_data[\"term\"].str.replace(\" months\", \"\")\n",
    "loan_data[\"term_int\"] = pd.to_numeric(loan_data[\"term_int\"])\n",
    "loan_data[\"term_int\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    466213.000000\n",
       "mean        300.482664\n",
       "std          93.976589\n",
       "min        -551.000000\n",
       "25%         244.000000\n",
       "50%         286.000000\n",
       "75%         346.000000\n",
       "max         648.000000\n",
       "Name: month_passed, dtype: float64"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#transform the string variable indicating date time\n",
    "loan_data['earliest_date'] = pd.to_datetime(loan_data['earliest_cr_line'], format = \"%b-%y\")\n",
    "loan_data['days_passed'] = pd.to_datetime('2022-12-31') - loan_data['earliest_date']\n",
    "loan_data['month_passed'] = round(pd.to_numeric((pd.to_datetime('2022-12-31')-loan_data['earliest_date'])/np.timedelta64(1, 'M')))\n",
    "loan_data['month_passed'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time since the credit issued can not be negative, so, let's check what is happening there\n",
    "loan_data[loan_data['month_passed'] < 0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above specification, we can see that we have measurement error in years. To be precise, earliest date the credit line issued is stated with future dates, such as '2065-05-27', which is obviously not the case. Since our data is rich enough, we will not investigate the causes of this error, and drop the values with month_passed being negative. This way, we can get rid of credits issued later than the real time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the following \n",
    "loan_data.drop(loan_data[loan_data['month_passed'] < 0].index, inplace=True)\n",
    "loan_data['month_passed'].describe()\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the same procedure for term and issue_date variables\n",
    "\n",
    "loan_data['earliest_date'] = pd.to_datetime(loan_data['earliest_cr_line'], format = \"%b-%y\")\n",
    "loan_data['days_passed'] = pd.to_datetime('2022-12-31') - loan_data['earliest_date']\n",
    "loan_data['month_passed'] = round(pd.to_numeric((pd.to_datetime('2022-12-31')-loan_data['earliest_date'])/np.timedelta64(1, 'M')))\n",
    "loan_data['month_passed'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(loan_data['term'])\n",
    "loan_data['term_int'] = pd.to_numeric(loan_data['term'].str.replace(' months', ''))\n",
    "term_dummies = pd.get_dummies(loan_data['term_int'], prefix='term')\n",
    "loan_data = pd.concat([loan_data, term_dummies], axis=1)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the term variable has only two inputs (36 and 60) representing months, we can confidently consider this as categorical variables. get_dummies command is creating two dummy variables. term_36 is equal to 1, when term is equal to 36 month, and 0 otherwise. The same definition is valid for term_60 variable as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the issue date from end of 2022\n",
    "\n",
    "loan_data['issue_d_date'] = pd.to_datetime(loan_data['issue_d'], format = '%b-%y')\n",
    "loan_data['months_issue_d'] = round(pd.to_numeric((pd.to_datetime('2022-12-31') - loan_data['issue_d_date']) / np.timedelta64(1, 'M')))\n",
    "loan_data['months_issue_d'].describe()\n",
    "\n",
    "# the minimum of the months issued seems quite reasonable, therefore, we do not need to further make changes in this variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing some discrete variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy variables for gender of the clients\n",
    "all_dummies = [ pd.get_dummies(loan_data['grade'], prefix = 'grade'),\n",
    "                pd.get_dummies(loan_data['sub_grade'], prefix = 'sub_grade'),\n",
    "                pd.get_dummies(loan_data['home_ownership'], prefix= 'home_ownership'),\n",
    "                pd.get_dummies(loan_data['verification_status'], prefix = 'verif_status'),\n",
    "                pd.get_dummies(loan_data['loan_status'], prefix = 'loan_status'),\n",
    "                pd.get_dummies(loan_data['purpose'], prefix = 'purpose'),\n",
    "                pd.get_dummies(loan_data['addr_state'], prefix = 'addr_state'),\n",
    "                pd.get_dummies(loan_data['initial_list_status'], prefix = 'initial_list_status')]\n",
    "all_dummies = pd.concat(all_dummies, axis=1)\n",
    "\n",
    "# concatenate the new dummy variables to the main dataset\n",
    "loan_data = pd.concat([loan_data, all_dummies], axis=1)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step creates dummy variables for mentioned categorical variables, such as gender, loan or verification status, or home ownership. In further modeling, these variables are treated separately. In other words, type of the ownership the client has in terms of housing could be rent, mortgage or his/her own, and these dummy variables help us estimate each of these home ownership impacts on credit risk. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows=None\n",
    "loan_data.isnull().sum()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dealing with missing values (MV) in credit risk modeling can have a significant impact on model accuracy and performance. Depending on the context of each variables, we can come up with a way to deal with missing values. For example, missing values in maximum revolving amount could be replaced by the loan amount itself, meaning the limit is the loan amount per se. Employment title (emp_title) has very large number of missing values, and this has been categorized among more than 17 thousand categories. Therefore, this variable is not likely to become our main variables of interest. Dropping unimportant missing variables decreases the sample size, but has no benefits after all, sp we leave it as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MVs revolving limit is replaced by funded amount\n",
    "loan_data['total_rev_hi_lim'].fillna(loan_data['funded_amnt'], inplace=True)\n",
    "\n",
    "# MVs in annual income is replaced by mean of annual income \n",
    "loan_data['annual_inc'].fillna(loan_data['annual_inc'].mean(), inplace=True)\n",
    "\n",
    "# MVs of the rest of the variables are replaced by zero based on their nature\n",
    "loan_data['months_issue_d'].fillna(0, inplace=True)\n",
    "loan_data['acc_now_delinq'].fillna(0, inplace=True)\n",
    "loan_data['total_acc'].fillna(0, inplace=True)\n",
    "loan_data['pub_rec'].fillna(0, inplace=True)\n",
    "loan_data['open_acc'].fillna(0, inplace=True)\n",
    "loan_data['inq_last_6mths'].fillna(0, inplace=True)\n",
    "loan_data['delinq_2yrs'].fillna(0, inplace=True)\n",
    "loan_data['emp_length_int'].fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model building "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create dummy variables to indicate whether a continuous variable falls within certain intervals, but this is not a typical approach for logistic regression. Logistic regression is designed to model the relationship between a binary response variable and one or more predictor variables, where the predictors can be continuous, categorical, or a combination of both.\n",
    "\n",
    "In the case of a continuous predictor variable like income, it is more common to use the raw income values directly in the logistic regression model, rather than transforming them into dummy variables based on intervals. This is because the relationship between income and the response variable may be non-linear, and splitting income into discrete intervals may lead to loss of information and decrease the accuracy of the model.\n",
    "\n",
    "That being said, in some cases, creating interval dummy variables may be useful if the relationship between the response variable and the predictor variable is not linear, and if the intervals are based on a priori knowledge or domain expertise. In these cases, the dummy variables can be included in the logistic regression model along with the raw income values to capture the non-linear relationship. However, this approach should be used with caution, as it can result in overfitting if the number of intervals is large."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin building the model, we need to specify the definition of default. We have loan status variable indicating the the loan status, whether it has fully been paid, charged off, defaulted, delaying the payment up to 120 days and so forth. \n",
    "\n",
    "In the following step, qualitative variable indicating if the loan status is good or bad is assigned with dummy variable. 1 represents the payment quality being good, that is, the loan is paid off in time, while 0 means all categories that represents critical situation with payments: default, charged off, payment is delayed for 31-120 days and not meeting the credit policy. This variable is further used in logistic and other regression models as dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows=50\n",
    "pd.options.display.max_columns=None\n",
    "loan_data['loan_status'].value_counts()\n",
    "loan_data[\"payment_quality\"] = np.where(loan_data['loan_status'].isin(['Charged Off', 'Default', \n",
    "                                                                        'Does not meet the credit policy. Status_Charged Off',\n",
    "                                                                        'Late (31-120 days)',]), 0, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independent variables \n",
    "We can group the independent variables into two categories. Categorical (discrete) and continuous variables. We group the continuous variables into categorical variables, such as income and debt using \"Weight of Evidence\" method. This is  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_test_split(loan_data.drop('payment_quality', axis = 1), loan_data['payment_quality'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data_train_inputs, loan_data_test_inputs, loan_data_train_targets, loan_data_test_targets = train_test_split(loan_data.drop('payment_quality',\n",
    "                                                                                                                  axis = 1), \n",
    "                                                                                                                  loan_data['payment_quality'],\n",
    "                                                                                                                  test_size=0.25,\n",
    "                                                                                                                  random_state=42)\n",
    "shapes = [  loan_data_train_inputs.shape,\n",
    "            loan_data_train_targets.shape,\n",
    "            loan_data_test_inputs.shape,\n",
    "            loan_data_test_targets.shape]\n",
    "\n",
    "shapes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = loan_data_train_inputs\n",
    "targets = loan_data_train_targets\n",
    "\n",
    "df=pd.concat([inputs['grade'], targets], axis = 1)\n",
    "\n",
    "df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].count()\n",
    "\n",
    "df = pd.concat([df.groupby(df.columns.values[0], as_index=False)[df.columns.values[1]].count(),\n",
    "                    df.groupby(df.columns.values[0], as_index=False)[df.columns.values[1]].mean()], axis = 1 )\n",
    "\n",
    "df = df.iloc[:,[0,1,3]]\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight of Evidence and Information Value\n",
    "Weight of Evidence (WOE) and Information Value (IV) are two statistical measures used in credit scoring and predictive modeling to evaluate the power of a predictor in explaining the target variable.\n",
    "\n",
    "Weight of Evidence (WOE) is a measure of how well a predictor separates the positive (good) cases from the negative (bad) cases. It is the logarithmic transformation of the odds ratio and is expressed as the difference between the natural logarithm of the event rate for the positive cases and the event rate for the negative cases. WOE helps in transforming the original predictor into a new predictor that is better at separating the positive and negative cases.\n",
    "\n",
    "Information Value (IV) is a measure of the strength of association between a predictor and the target variable. It is a simple summary statistic that can be used to select the best predictors for a predictive model. IV is the sum of the weighted differences between the event rate for the positive cases and the event rate for the negative cases, where the weight is the proportion of observations in that group.\n",
    "\n",
    "In credit scoring, WOE and IV are used to select the best predictors for the credit scorecard, to transform the predictors so that they are better at separating the good and bad cases, and to evaluate the performance of the scorecard. In predictive modeling, they are used to identify the predictors that have the strongest association with the target variable and to transform the predictors so that they are better suited for building a predictive model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The formula for weight of evidence (WoE) is as following:\n",
    "$$\n",
    "WoE = ln\\left(\\frac{\\text{proportion of good events}}{\\text{proportion of bad events}}\\right)\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following woe_iv_dis function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def woe_iv_dis(df, varname, qualitative_var):\n",
    "    \"\"\"\n",
    "    Calculates the Weight of Evidence (WoE) and Information Value (IV) of a categorical variable.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing the target variable and the categorical variable.\n",
    "    varname (str): Name of the target variable.\n",
    "    qualitative_var (str): Name of the categorical variable.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing the WoE and IV values for each category of the categorical variable.\n",
    "    \n",
    "    The columns of the output DataFrame are:\n",
    "    - the categorical variable\n",
    "    - 'freq. of class': frequency of each category\n",
    "    - 'mean of class': mean value of the target variable for each category\n",
    "    - 'proportions': proportion of each category in the data\n",
    "    - 'n_good': number of good outcomes in each category\n",
    "    - 'n_bad': number of bad outcomes in each category\n",
    "    - 'prop_good': proportion of good outcomes in each category\n",
    "    - 'prop_bad': proportion of bad outcomes in each category\n",
    "    - 'weight_of_evidence': WoE value for each category\n",
    "    - 'delta_WoE': difference in absolute value of WoE between consecutive categories\n",
    "    - 'info_value': IV value for the categorical variable\n",
    "    \"\"\"\n",
    "    df = pd.concat([df[varname], qualitative_var], axis = 1)\n",
    "    df = pd.concat([df.groupby(df.columns.values[0], as_index=False)[df.columns.values[1]].count(),\n",
    "                    df.groupby(df.columns.values[0], as_index=False)[df.columns.values[1]].mean()], axis = 1 )\n",
    "\n",
    "    df = df.iloc[:, [0,1,3]]\n",
    "    df.columns = [df.columns.values[0], 'freq. of class', 'mean of class']\n",
    "    df['proportions'] = df['freq. of class'] / df['freq. of class'].sum()\n",
    "    df['n_good'] = df['mean of class'] * df['freq. of class']\n",
    "    df['n_bad'] = (1 - df['mean of class']) * df['freq. of class']\n",
    "    df['prop_good'] = df['n_good'] / df['n_good'].sum()\n",
    "    df['prop_bad'] = df['n_bad'] / df['n_bad'].sum()\n",
    "    df['weight_of_evidence'] = np.log(df['prop_good']/df['prop_bad'])\n",
    "    df = df.sort_values(['weight_of_evidence'])\n",
    "    df = df.reset_index(drop=True)\n",
    "    df['delta_WoE'] = df['weight_of_evidence'].diff().abs()\n",
    "    df['info_value'] = (df['prop_good'] - df['prop_bad']) * df['weight_of_evidence']\n",
    "    df['info_value'] = df['info_value'].sum()\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = loan_data_train_inputs\n",
    "targets = loan_data_train_targets\n",
    "\n",
    "df_test = woe_iv_dis(df=inputs, varname='grade',  qualitative_var= targets)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_woe(df_woe_iv, fig_size=(20,8), x_degree=0):\n",
    "    \"\"\"\"\"\"\n",
    "    x = np.array(df_woe_iv.iloc[:,0].apply(str))\n",
    "    y = df_woe_iv['weight_of_evidence']\n",
    "    plt.figure(figsize=fig_size)\n",
    "    plt.plot(x,y, marker = 'o', linestyle = '--', color = 'k')\n",
    "    plt.xlabel(df_woe_iv.columns[0])\n",
    "    plt.ylabel('Weight of Evidence')\n",
    "    plt.title(str('Weight of Evidence  ' + df_woe_iv.columns[0]))\n",
    "    plt.xticks(rotation = x_degree)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_woe(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades  = inputs['grade']\n",
    "grades_order = ['A', 'B', 'C', 'D', 'E', 'F']\n",
    "grades.sort_values()\n",
    "#count the grades\n",
    "grade_counts = {}\n",
    "for grade in grades:\n",
    "    if grade in grade_counts:\n",
    "        grade_counts[grade] +=1\n",
    "    else:\n",
    "        grade_counts[grade] = 1\n",
    "\n",
    "# plot the grade frequency\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(grades_order, [grade_counts[grade] for grade in grades_order])\n",
    "plt.xlabel('Grade category')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Frequency of grades in the Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the weight of evidence of the home ownership variable\n",
    "df_home_own = woe_iv_dis(inputs, 'home_ownership', targets)\n",
    "df_home_own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_woe(df_home_own, (16,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# home ownership categories \"other\", \"none\" and \"any\" can be combined\n",
    "inputs['home_own_none_other_any_combined'] = sum([inputs['home_ownership_ANY'], inputs['home_ownership_NONE'], inputs['home_ownership_OTHER']])\n",
    "\n",
    "''' this step leaves us only four categories: rent, mortgage, own, and others combined in one variable. \n",
    "Using unification technique we combine the three less informative variables into one which does not impact on final result very much, \n",
    "yet cuts computational costs'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate weights of evidences for location\n",
    "df_location = woe_iv_dis(inputs, 'addr_state', targets)\n",
    "df_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_woe(df_location, (16,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = inputs['addr_state']\n",
    "state_counts = {}\n",
    "for state in states:\n",
    "    if state in state_counts:\n",
    "        state_counts[state] += 1\n",
    "    else:\n",
    "        state_counts[state] = 1\n",
    "\n",
    "# Plot the frequency chart\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.bar(state_counts.keys(), state_counts.values())\n",
    "plt.xlabel('State Abbreviation')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Frequency of State Abbreviations in the Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping the states based on states to create classes\n",
    "\n",
    "This step of grouping the weights of evidence can be useful in credit risk modeling because it allows the model to group similar observations together. This can help to simplify the analysis and reduce the noise in the data, which can lead to more accurate predictions.\n",
    "\n",
    "In credit risk modeling, the weights of evidence are often used to indicate the likelihood of a borrower defaulting on a loan. By grouping similar weights of evidence together, the analyst can identify patterns and relationships that may not be immediately apparent when examining each weight of evidence individually. This can help to inform the development of more accurate and effective credit risk models.\n",
    "\n",
    "In the next several steps carry out the process of grouping the states. States with higher frequency in the data are taken as separate groups. States like California, New York, Florida and Texas are relatively more frequent than other states, therefore, each of these states constitute one group itself. For the rest, we employ the techniques to group them together and create classes based on weights of evidences.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the weights of evidence removing the outliers\n",
    "plot_woe(df_location.iloc[6:-6,:], (25,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 4 highly frequent states\n",
    "inputs[\"st_group_TX\"] = sum(inputs['addr_state_TX'])\n",
    "inputs[\"st_group_FL\"] = sum(inputs['addr_state_FL'])\n",
    "inputs[\"st_group_NY\"] = sum(inputs['addr_state_NY'])\n",
    "inputs[\"st_group_CA\"] = sum(inputs['addr_state_CA'])\n",
    "\n",
    "\n",
    "# The rest \n",
    "inputs['st_group_NM_MD_NC_LA_MD'] = sum([inputs['addr_state_NM'],inputs['addr_state_MD'],inputs['addr_state_NC'], inputs['addr_state_LA'],\n",
    "                                                inputs['addr_state_MD']])\n",
    "\n",
    "inputs['st_group_MI_NJ_VA'] = sum([inputs['addr_state_MI'],inputs['addr_state_NJ'],inputs['addr_state_VA']])\n",
    "\n",
    "inputs['st_group_OK_TN_AZ_DE_AR_UT'] = sum([inputs['addr_state_OK'],inputs['addr_state_TN'],inputs['addr_state_AZ'], inputs['addr_state_DE'],\n",
    "                                                inputs['addr_state_AR'],inputs['addr_state_UT']])\n",
    "\n",
    "inputs['st_group_KY_MN_NA_IN_OH'] = sum([inputs['addr_state_KY'],inputs['addr_state_MN'],inputs['addr_state_MA'],inputs['addr_state_IN'],\n",
    "                                        inputs['addr_state_OH']])\n",
    "\n",
    "inputs['st_group_RI_OR_GA_WA'] = sum([inputs['addr_state_RI'],inputs['addr_state_OR'],inputs['addr_state_GA'], inputs['addr_state_WA']])\n",
    "\n",
    "inputs['st_group_SD_ID'] = sum([inputs['addr_state_SD'],inputs['addr_state_ID']])\n",
    "\n",
    "inputs['st_group_MS_MT'] = sum([inputs['addr_state_MS'], inputs['addr_state_MT']])\n",
    "\n",
    "inputs['st_group_IL_CT_CO'] = sum([inputs['addr_state_IL'], inputs['addr_state_CT'], inputs['addr_state_CT'], inputs['addr_state_CO']])\n",
    "\n",
    "inputs['st_group_VT_SC'] = sum([inputs['addr_state_VT'], inputs['addr_state_SC']])\n",
    "\n",
    "inputs['st_group_KS'] = sum([inputs['addr_state_KS']])\n",
    "\n",
    "\n",
    "\n",
    "inputs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[\"term:36\"] = np.where((inputs['term_int']==36),1,0)\n",
    "inputs[\"term:60\"] = np.where((inputs['term_int']==60),1,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ver_status = woe_iv_dis(inputs, 'verification_status', targets)\n",
    "df_purpose = woe_iv_dis(inputs, \"purpose\", targets)\n",
    "df_init_status = woe_iv_dis(inputs, \"initial_list_status\", targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_woe(df_ver_status, (16,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_woe(df_purpose, (20,8), x_degree=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_woe(df_init_status, (18,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def woe_iv_cont(df, varname, qualitative_var):\n",
    "    \"\"\"\n",
    "    Calculates the Weight of Evidence (WoE) and Information Value (IV) of a categorical variable.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing the target variable and the categorical variable.\n",
    "    varname (str): Name of the target variable.\n",
    "    qualitative_var (str): Name of the categorical variable.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing the WoE and IV values for each category of the categorical variable.\n",
    "    \n",
    "    The columns of the output DataFrame are:\n",
    "    - the categorical variable\n",
    "    - 'freq. of class': frequency of each category\n",
    "    - 'mean of class': mean value of the target variable for each category\n",
    "    - 'proportions': proportion of each category in the data\n",
    "    - 'n_good': number of good outcomes in each category\n",
    "    - 'n_bad': number of bad outcomes in each category\n",
    "    - 'prop_good': proportion of good outcomes in each category\n",
    "    - 'prop_bad': proportion of bad outcomes in each category\n",
    "    - 'weight_of_evidence': WoE value for each category\n",
    "    - 'delta_WoE': difference in absolute value of WoE between consecutive categories\n",
    "    - 'info_value': IV value for the categorical variable\n",
    "    \"\"\"\n",
    "    df = pd.concat([df[varname], qualitative_var], axis = 1)\n",
    "    df = pd.concat([df.groupby(df.columns.values[0], as_index=False)[df.columns.values[1]].count(),\n",
    "                    df.groupby(df.columns.values[0], as_index=False)[df.columns.values[1]].mean()], axis = 1 )\n",
    "\n",
    "    df = df.iloc[:, [0,1,3]]\n",
    "    df.columns = [df.columns.values[0], 'freq. of class', 'mean of class']\n",
    "    df['proportions'] = df['freq. of class'] / df['freq. of class'].sum()\n",
    "    df['n_good'] = df['mean of class'] * df['freq. of class']\n",
    "    df['n_bad'] = (1 - df['mean of class']) * df['freq. of class']\n",
    "    df['prop_good'] = df['n_good'] / df['n_good'].sum()\n",
    "    df['prop_bad'] = df['n_bad'] / df['n_bad'].sum()\n",
    "    df['weight_of_evidence'] = np.log(df['prop_good']/df['prop_bad'])\n",
    "    \n",
    "    # Unlike previous weight of evidence function, we do not sort the variable by weo, but instead, keep the variable's natural order\n",
    "\n",
    "    df['delta_WoE'] = df['weight_of_evidence'].diff().abs()\n",
    "    df['info_value'] = (df['prop_good'] - df['prop_bad']) * df['weight_of_evidence']\n",
    "    df['info_value'] = df['info_value'].sum()\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# employement length\n",
    "\n",
    "df_emp_l = woe_iv_cont(inputs, 'emp_length_int', targets)\n",
    "plot_woe(df_emp_l, (16,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emp_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['emp_length:0'] = np.where(inputs['emp_length'].isin([0]), 1, 0)\n",
    "inputs['emp_length:1'] = np.where(inputs['emp_length'].isin([1]), 1, 0)\n",
    "inputs['emp_length:2-4'] = np.where(inputs['emp_length'].isin(range(2,5)), 1, 0)\n",
    "inputs['emp_length:5-6'] = np.where(inputs['emp_length'].isin(range(5,7)), 1, 0)\n",
    "inputs['emp_length:7-9'] = np.where(inputs['emp_length'].isin(range(7,10)), 1, 0)\n",
    "inputs['emp_length:10'] = np.where(inputs['emp_length'].isin([10]), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since months_issued variable has a lot of classes, we have to decrease this number to make it easier to include in the model\n",
    "inputs['months_factor'] = pd.cut(inputs['months_issue_d'], 10) \n",
    "inputs['months_factor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_months = woe_iv_cont(inputs, 'months_factor', targets)\n",
    "df_months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_woe(df_months, x_degree=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using graphical representation of the number of months passed since loan issued, we can create classes for all of the trends\n",
    "inputs['months_since_issued:106'] = np.where(inputs['months_factor'].isin(range(95,106)), 1,0)\n",
    "inputs['months_since_issued:115'] = np.where(inputs['months_factor'].isin(range(106,115)), 1,0)\n",
    "inputs['months_since_issued:124'] = np.where(inputs['months_factor'].isin(range(115,124)), 1,0)\n",
    "inputs['months_since_issued:133'] = np.where(inputs['months_factor'].isin(range(124,133)), 1,0)\n",
    "inputs['months_since_issued:142'] = np.where(inputs['months_factor'].isin(range(133,142)), 1,0)\n",
    "inputs['months_since_issued:151'] = np.where(inputs['months_factor'].isin(range(142,151)), 1,0)\n",
    "inputs['months_since_issued:160'] = np.where(inputs['months_factor'].isin(range(151,160)), 1,0)\n",
    "inputs['months_since_issued:169'] = np.where(inputs['months_factor'].isin(range(160,169)), 1,0)\n",
    "inputs['months_since_issued:178'] = np.where(inputs['months_factor'].isin(range(169,178)), 1,0)\n",
    "inputs['months_since_issued:187'] = np.where(inputs['months_factor'].isin(range(178,187)), 1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['int_rate_classes'] = pd.cut(inputs['int_rate'], 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_interest = woe_iv_cont(inputs, 'int_rate_classes', targets)\n",
    "plot_woe(df_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['int_rate:9.5'] = np.where((inputs['int_rate_classes'].cat.codes < 9.548), 1,0)\n",
    "inputs['int_rate:13.6'] = np.where((inputs['int_rate_classes'].cat.codes<=13.676) & (inputs['int_rate_classes'].cat.codes > 9.548), 1,0)\n",
    "inputs['int_rate:17.8'] = np.where((inputs['int_rate_classes'].cat.codes <=17.807) & (inputs['int_rate_classes'].cat.codes > 13.676) , 1,0)\n",
    "inputs['int_rate:26.06'] = np.where((inputs['int_rate_classes'].cat.codes <=26.06) & (inputs['int_rate_classes'].cat.codes > 19.86), 1,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['funded_amount_class'] = pd.cut(inputs['funded_amnt'],50)\n",
    "df_funded = woe_iv_cont(inputs, 'funded_amount_class', targets)\n",
    "plot_woe(df_funded, x_degree=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annual income - one of the most important variables in the model has to be carefully preprocessed "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['income_classes'] = pd.cut(inputs['annual_inc'], 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_income = woe_iv_cont(inputs, 'income_classes', targets)\n",
    "df_income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "max_income_class = inputs['income_classes'].max()\n",
    "inputs = inputs.reset_index(drop=True)\n",
    "inputs['income_classes'] = inputs['income_classes'][inputs['income_classes'] != max_income_class]\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs['income_classes'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(inputs['annual_inc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_income_observation = inputs.loc[inputs['annual_inc'].idxmax()]\n",
    "print(max_income_observation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = inputs.reset_index(drop=True)\n",
    "df_income = woe_iv_cont(inputs, 'income_classes', targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "inputs['annual_inc'].plot(kind='kde')\n",
    "\n",
    "# Set the plot title and axis labels\n",
    "plt.title('Density Plot of Annual Income')\n",
    "plt.xlabel('Annual Income')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_woe(df_income)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb4569285eef3a3450cb62085a5b1e0da4bce0af555edc33dcf29baf3acc1368"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
